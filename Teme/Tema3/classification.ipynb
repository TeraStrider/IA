{
 "cells": [
  {
   "source": [
    "# Cerinta 1 - Reprezentarea modelului de clasificare"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/teo/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/teo/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from json import dumps\n",
    "from math import inf, log\n",
    "from operator import add\n",
    "from os import listdir\n",
    "from random import shuffle\n",
    "\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy import load\n",
    "\n",
    "download('wordnet')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassificationModel:\n",
    "\tdef __init__(self, stop_words_file):\n",
    "\t\tself._ALPHA = 1\n",
    "\t\tself._chars_to_remove = '012345679.,-~`|\\\\/:;\\'\"?![]()\\n\\\"'\n",
    "\t\tself.class_words = {}\n",
    "\t\tself.class_app = {}\n",
    "\t\tself.total_files = 0\n",
    "\t\tself.stop_pos = {'PART', 'DET', 'PUNCT', 'AUX'}\n",
    "\n",
    "\t\tself.wnl = WordNetLemmatizer()\n",
    "\t\tself.nlp = load('en')\n",
    "\n",
    "\t\tself.stop_words = set(stopwords.words('english')) | set(' ')\n",
    "\t\twith open(stop_words_file) as f:\n",
    "\t\t\tself.stop_words.union(set(token.replace('\\n', '') for token in f))\n",
    "\n",
    "\n",
    "\tdef _remove_chars(self, s):\n",
    "\t\tfor ch in self._chars_to_remove:\n",
    "\t\t\ts = s.replace(ch, '')\n",
    "\n",
    "\t\treturn s\n",
    "\n",
    "\n",
    "\tdef _parse_file(self, file):\n",
    "\t\twords = []\n",
    "\t\t# TODO: fa mai functional\n",
    "\t\twith open(file, encoding='utf8') as f:\n",
    "\t\t\tfor tok in self.nlp(f.read().replace('\\n', ' ')):\n",
    "\t\t\t\tif (tok.lemma_ not in self.stop_words\n",
    "\t\t\t\t\tand tok.pos_ not in self.stop_pos\n",
    "\t\t\t\t\tand not tok.is_stop\n",
    "\t\t\t\t):\n",
    "\t\t\t\t\tstripped_lem = self._remove_chars(tok.lemma_).lower()\n",
    "\t\t\t\t\tif stripped_lem:\n",
    "\t\t\t\t\t\twords.append(self.wnl.lemmatize(stripped_lem))\n",
    "\n",
    "\t\treturn filter(lambda l: l not in self.stop_words, words)\n",
    "\n",
    "\n",
    "\tdef add_file(self, file, clss):\n",
    "\t\tself.total_files += 1\n",
    "\n",
    "\t\tif clss in self.class_app:\n",
    "\t\t\tself.class_app[clss] += 1\n",
    "\t\telse:\n",
    "\t\t\tself.class_app[clss] = 1\n",
    "\t\t\tself.class_words[clss] = {}\n",
    "\n",
    "\t\tfor lem in self._parse_file(file):\n",
    "\t\t\tif lem in self.class_words[clss]:\n",
    "\t\t\t\tself.class_words[clss][lem] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.class_words[clss][lem] = 1\n",
    "\n",
    "\n",
    "\tdef _compute_log_prob(self, word, cl):\n",
    "\t\treturn log(\n",
    "\t\t\t(self.class_words[cl].get(word, 0) + self._ALPHA)\n",
    "\t\t\t/ (len(self.class_words[cl]) + len(self.class_words) * self._ALPHA)\n",
    "\t\t)\n",
    "\n",
    "\n",
    "\tdef run_inference(self, file):\n",
    "\t\tmax_prob = -inf\n",
    "\t\tlems = list(self._parse_file(file))  # TODO: incearca fara list()\n",
    "\n",
    "\t\tfor cl in self.class_words:\n",
    "\t\t\tcl_prob = reduce(\n",
    "\t\t\t\tadd,\n",
    "\t\t\t\tmap(lambda l: self._compute_log_prob(l, cl), lems),\n",
    "\t\t\t\t0\n",
    "\t\t\t) + log(self.class_app[cl] / self.total_files)\n",
    "\n",
    "\t\t\tif max_prob < cl_prob:\n",
    "\t\t\t\tmax_prob = cl_prob\n",
    "\t\t\t\tpred = cl\n",
    "\t\t\n",
    "\t\treturn pred\n",
    "\n",
    "\tdef print(self, file):\n",
    "\t\twith open(file, 'w') as f:\n",
    "\t\t\tf.write(\"class_app:\\n\")\n",
    "\t\t\tf.write(dumps(self.class_app, indent=4) + '\\n')\n",
    "\t\t\tf.write(\"class_words:\\n\")\n",
    "\t\t\tf.write(dumps(self.class_words, indent=4) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_model(path, stop_words_file):\n",
    "    model = NewsClassificationModel(stop_words_file)\n",
    "    test_files = {}\n",
    "\n",
    "    for clss in listdir(path):\n",
    "        files_in_class = listdir(f'{path}/{clss}')\n",
    "        shuffle(files_in_class)\n",
    "        \n",
    "        num_files = len(files_in_class)\n",
    "\n",
    "        test_files[clss] = files_in_class[:int(num_files / 4)]\n",
    "        train_files = files_in_class[int(num_files / 4):]\n",
    "\n",
    "        [model.add_file(f'{path}/{clss}/{f}', clss) for f in train_files]\n",
    "    \n",
    "    return model, test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(path, stop_words_file):\n",
    "    model, test_files = create_classification_model(path, stop_words_file)\n",
    "    total_preds = {clss: 0 for clss in test_files}\n",
    "    correct_preds = {}\n",
    "    recalls = {}\n",
    "\n",
    "    model.print('model_spacey')\n",
    "\n",
    "    for cl, files in test_files.items():\n",
    "        preds = list(map(lambda f: model.run_inference(f'{path}/{cl}/{f}'), files))\n",
    "        correct_preds[cl] = len(list(filter(lambda c: c == cl, preds)))\n",
    "        recalls[cl] = correct_preds[cl] / len(files)\n",
    "\n",
    "        # TODO: cauta cum sa faci functional\n",
    "        for pred in preds:\n",
    "            total_preds[pred] += 1\n",
    "    \n",
    "    for clss in test_files:\n",
    "        print(f'====== {clss} ======')\n",
    "        print(f'Recall = {recalls[clss]}')\n",
    "        print(f'Precision = {correct_preds[clss] / total_preds[clss]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====== business ======\nRecall = 0.9291338582677166\nPrecision = 0.9915966386554622\n\n====== entertainment ======\nRecall = 0.8125\nPrecision = 1.0\n\n====== politics ======\nRecall = 0.9903846153846154\nPrecision = 0.8956521739130435\n\n====== sport ======\nRecall = 1.0\nPrecision = 1.0\n\n====== tech ======\nRecall = 1.0\nPrecision = 0.8695652173913043\n\n"
     ]
    }
   ],
   "source": [
    "test_classification('BBC News Summary/News Articles', 'stop_words')"
   ]
  },
  {
   "source": [
    "# Cele mai bune rezultate - Cerinta 2\n",
    "```\n",
    "====== business ======\n",
    "Recall = 0.952755905511811\n",
    "Precision = 0.9918032786885246\n",
    "\n",
    "====== entertainment ======\n",
    "Recall = 0.8541666666666666\n",
    "Precision = 1.0\n",
    "\n",
    "====== politics ======\n",
    "Recall = 0.9807692307692307\n",
    "Precision = 0.9272727272727272\n",
    "\n",
    "====== sport ======\n",
    "Recall = 0.9921259842519685\n",
    "Precision = 1.0\n",
    "\n",
    "====== tech ======\n",
    "Recall = 1.0\n",
    "Precision = 0.8771929824561403\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}